import scanpy as sc
import anndata
import palantir
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns
sns.set(font_scale=2)


def get_compactness(ad, per_metacell=True, low_density_cells=False, nth_neighbor=15):
    """
    Returns compactness of each metacell or each cell by measuring the negative log-likelihood of each cell being
    generated by its metacell (average over metacell if computed per-metacell). See get_NLLs() for a description of how
    negative log-likelihoods are computed.

    :param ad: (anndata.AnnData) object containing Metacell assignments and data embedding in ad.obsm
    :param per_metacell: (bool) whether to aggregate compactness scores per metacell
    :param low_density_cells: (bool) whether to compute compactness on only the least dense cells (lower tertile in
                                density
    :return: pd.DataFrame with either cell ID or metacell ID as index, and compactness as the column
    """
    if ('metacell_metrics' not in ad.uns) or ('NLLs' not in ad.uns['metacell_metrics']):
        # We may be able to run on get_NLLs automatically...
        if ('X_pca' in ad.obsm) and ('X_svd' in ad.obsm):
            raise Exception('Need to run evaluate.get_NLLs() before compactness can be computed.')
        elif 'X_pca' in ad.obsm:
            print('Computing negative log-likelihoods with X_pca.')
            get_NLLs(ad, key='X_pca')
        elif 'X_svd' in ad.obsm:
            print('Computing negative log-likelihoods with X_pca.')
            get_NLLs(ad, key='X_svd')
        else:
            raise Exception('Need to run evaluate.get_NLLs() before compactness can be computed.')

    ad.uns['metacell_metrics']['NLLs']['compactness'] = ad.uns['metacell_metrics']['NLLs']['self']


    if low_density_cells:
        if 'density' not in ad.uns['metacell_metrics']:
            print('Computing cell densities.')
            get_density(ad, nth_neighbor=nth_neighbor)

        low_density_cells = ad.uns['metacell_metrics']['density'][ad.uns['metacell_metrics']['density']['mc_density_tertile'] == 0].index
        df = ad.uns['metacell_metrics']['NLLs'].loc[low_density_cells]
    else:
        df = ad.uns['metacell_metrics']['NLLs']

    if per_metacell:
        # Aggregate per metacell and return
        return df.groupby('Metacell').mean()[['compactness']]

    return df[['compactness']]


def get_separation(ad, nbr=1, per_metacell=True, low_density_cells=False, nth_neighbor=150):
    """
    Returns separation of each metacell or each cell by measuring the difference in negative log-likelihood of each cell
    being generated by a neighbouring metacell versus its metacell (average over metacell if computed per-metacell). The
    neighbour is specified by the argument 'nbr', e.g. nbr=1 means first nearest neighbour etc.

    See get_NLLs() for a description of how negative log-likelihoods are computed.

    :param ad: (anndata.AnnData) object containing Metacell assignments and data embedding in ad.obsm
    :param nbr: (int) specifies rank of nearest neighbor to compare against.
    :param per_metacell: (bool) whether to aggregate compactness scores per metacell
    :param low_density_cells: (bool) whether to compute compactness on only the least dense cells (lower tertile in
                                density
    :return: pd.DataFrame with either cell ID or metacell ID as index, and separation as the column
    """
    if ('metacell_metrics' not in ad.uns) or ('NLLs' not in ad.uns['metacell_metrics']):
        # We may be able to run on get_NLLs automatically...
        if ('X_pca' in ad.obsm) and ('X_svd' in ad.obsm):
            raise Exception('Need to run evaluate.get_NLLs() before compactness can be computed.')
        elif 'X_pca' in ad.obsm:
            print('Computing negative log-likelihoods with X_pca.')
            get_NLLs(ad, key='X_pca')
        elif 'X_svd' in ad.obsm:
            print('Computing negative log-likelihoods with X_pca.')
            get_NLLs(ad, key='X_svd')
        else:
            raise Exception('Need to run evaluate.get_NLLs() before compactness can be computed.')

    # Get NLLs
    try:
        nbr_nlls = ad.uns['metacell_metrics']['NLLs'][nbr]
    except:
        raise Exception(f"Neighbor {nbr} could not be found as a column in .uns['metacell_metrics']['NLLs'].")

    ad.uns['metacell_metrics']['NLLs']['separation'] = nbr_nlls - ad.uns['metacell_metrics']['NLLs']['self']

    if low_density_cells:
        if 'density' not in ad.uns['metacell_metrics']:
            print('Computing cell densities.')
            get_density(ad, nth_neighbor=nth_neighbor)

        low_density_cells = ad.uns['metacell_metrics']['density'][ad.uns['metacell_metrics']['density']['mc_density_tertile'] == 0].index
        df = ad.uns['metacell_metrics']['NLLs'].loc[low_density_cells]
    else:
        df = ad.uns['metacell_metrics']['NLLs']

    if per_metacell:
        # Aggregate per metacell and return
        return df.groupby('Metacell').mean()[['separation']]
    return df[['separation']]

def get_NLLs(ad, 
             key = 'X_pca',
             n_neighbours = 5,
             train_split = 1,
             verbose = True):
    """
    Compute the NLL of each cell under the Gaussian defined by cells belonging to (1) its own metacell
    and (2) the metacells of its nearest neighbours. The multivariate Gaussian is defined in the space specified by key
    and distances are computed in the diffusion component space.

    Modifies the ad.uns['metacell_metrics'] dictionary to add a key 'NLLs' which contains
    output of this function.

    :param ad: (anndata.AnnData) object containing Metacell assignments and data embedding in ad.obsm
    :param key: (str) key in ad.obs used to define Multivariate Gaussian for each metacell
    :param n_neighbours: (int) number of nearest neighbours for which NLL is computing
    :param train_split: (float) proportion of cells to use as training data
    :param verbose:
    :return: pd.DataFrame containing the following columns:
            'self' - NLL of each cell under assigned metacell
            'train' - boolean describing whether each cell was used to fit the Gaussian as a training point
                        or whether it was used as a test point
            'Metacell' - containing metacell assignment
            k = 1 ... n_neighbours - containing the NLL of each cell under the kth nearest neighbour.
    """

    assert train_split <=1 and train_split >=0, 'train_split must lie in range [0,1] inclusive.'

    label_df = ad.obs[['Metacell']]
    label_df.loc[:, 'count_assist'] = 1
    cts = label_df.groupby('Metacell').count()
    
    if train_split < 1:
        if verbose:
            print(f'Clipping to metacells with at least {1/(1-train_split)} cells')
        sufficient = cts[cts.iloc[:, 0]>1/(1-train_split)].index
    else:
        if verbose:
            print(f'Clipping to metacells with at least 3 cells.')
        # Require at least 3 cells per metacell
        sufficient = cts[cts.iloc[:, 0]>=3].index

    if verbose:
        print(f'Dropping {len(label_df.Metacell.unique())-len(sufficient)} metacell(s) due to insufficient size.')

    dropped = label_df[~label_df['Metacell'].isin(sufficient)]

    label_df = label_df[label_df['Metacell'].isin(sufficient)]
    label_df['Metacell'] = label_df['Metacell'].astype(str).astype('category')

    components = pd.DataFrame(ad.obsm[key]).set_index(ad.obs_names)
    components = components.loc[label_df.index]

    # Compute neighbours in diffusion component space
    if verbose:
        print(f'Computing {n_neighbours} neighbours in diffusion component space and Gaussian from {key} space.')
    dm_res = palantir.utils.run_diffusion_maps(components)
    dc = palantir.utils.determine_multiscale_space(dm_res, n_eigs=10)

    # Compute DC per metacell
    metacells_dcs = dc.join(label_df.Metacell, how='inner').groupby('Metacell').mean()

    from sklearn.neighbors import NearestNeighbors
    neigh = NearestNeighbors(n_neighbors=n_neighbours)
    nbrs = neigh.fit(metacells_dcs)
    dists, nbrs = nbrs.kneighbors()
    dists = pd.DataFrame(dists).set_index(metacells_dcs.index)

    nbr_cells = np.array(metacells_dcs.index)[nbrs]

    metacells_nbrs = pd.DataFrame(nbr_cells)
    metacells_nbrs.index = metacells_dcs.index
    metacells_nbrs.columns += 1

    from scipy.spatial.distance import pdist, squareform

    mc_components_mean = components.merge(label_df['Metacell'], left_index=True, right_index=True).groupby('Metacell').mean()

    mc_dists = pd.DataFrame(squareform(pdist(mc_components_mean)))
    mc_dists.index = mc_components_mean.index
    mc_dists.columns = mc_components_mean.index

    dists = []
    for mc, row in metacells_nbrs.iterrows():
        dists.append(mc_dists[row].loc[mc].values)

    nbr_dists = pd.DataFrame(dists).set_index(metacells_nbrs.index)
    nbr_dists.columns += 1

    neighbours = pd.concat({'Metacell':metacells_nbrs, 'Distance':nbr_dists}, axis=1)

    from scipy.stats import multivariate_normal as mv_g
    mc_params = {}
    mc_splits = {}

    for mc in label_df['Metacell'].dropna().unique():
        # Subset to cells assigned to this metacell
        subset = label_df[label_df['Metacell']==mc].index

        # Choose a train/test subset 
        ix = np.arange(len(subset))
        np.random.shuffle(ix)
        train = int(train_split*len(subset))
        train_ix = subset[ix[:train]]
        test_ix = subset[ix[train:]]

        components_subset = components.loc[train_ix]
        # Learn a multivariate gaussian from the train set

        cov = components_subset.cov().values
        mean = np.mean(components_subset.values, axis=0)

        mc_params[mc] = (mean,cov)
        mc_splits[mc] = (train_ix, test_ix)

    NLL_df = {'cell_id':[], 'self': [], 'train':[], 'Metacell': []}

    for col in range(n_neighbours):
        NLL_df[col+1] = []

    # Now iterate through all metacells/ neighbours and compute separation and 
    for index, row in metacells_nbrs.iterrows():
        # Subset to components for just this metacell
        train_ix, test_ix = mc_splits[index]
        if train_split < 1:
            splits = [(train_ix, True), (test_ix, False)]
        else:
            splits = [(train_ix, True)]

        # Get individual logliks:
        for ix, truth in splits:
            components_subset = components.loc[ix]

            NLL_df['Metacell'] += [index]*len(components_subset)
            NLL_df['cell_id'] += list(components_subset.index)
            NLL_df['train'] += [truth]*len(components_subset)
            mean, cov = mc_params[index]

            # Compute the probability under the assigned cell
            gaussian = mv_g(mean, cov, allow_singular=True)
            NLL_df['self'] += list(-gaussian.logpdf(components_subset))

            for col, nbr in row.iteritems():
                mean, cov = mc_params[nbr]
                gaussian = mv_g(mean, cov, allow_singular=True)
                NLL_df[col] += list(-gaussian.logpdf(components_subset))
    NLL_df = pd.DataFrame(NLL_df).set_index('cell_id')

    if 'metacell_metrics' not in ad.obsm:
        ad.uns['metacell_metrics'] = {'NLLs':NLL_df}
    else:
        ad.uns['metacell_metrics']['NLLs'] = NLL_df

    return neighbours, NLL_df


def get_density(ad, nth_neighbor=15):
    """
    Compute cell density as 1/ the distance to the 15th (by default) nearest neighbour
    Modifies the ad.uns['metacell_metrics'] dictionary to add a key 'density' which contains
    output of this function.

    :param ad:
    :param nth_neighbor:
    :return: pd.DataFrame containing the following columns:
        'Metacell' - Metacell label for each cell
        'density' - density of each cell
        'tertile' - which tertile (0=lowers, 2=highest) each cell belongs to in terms of individual density
        'mc_density' - average density of the metacell each cell belongs to
        'mc_density_tertile' - which tertile (0=lowers, 2=highest) each cell belongs to in terms of metacell density
    """
    from sklearn.neighbors import NearestNeighbors
    neigh = NearestNeighbors(n_neighbors=nth_neighbor)
    
    if 'X_pca' in ad.obsm:
        print('Using PCA')
        pca_components = pd.DataFrame(ad.obsm['X_pca']).set_index(ad.obs_names)
    else:
        print('Using SVD')
        pca_components = pd.DataFrame(ad.obsm['X_svd']).set_index(ad.obs_names)
   
        
    dm_res = palantir.utils.run_diffusion_maps(pca_components)
    dc = palantir.utils.determine_multiscale_space(dm_res, n_eigs=8)

    nbrs = neigh.fit(dc)
    cell_density = pd.DataFrame(nbrs.kneighbors()[0][:,nth_neighbor-1]).set_index(ad.obs_names).rename(columns={0:'density'})
    density = 1/cell_density
    
    density['tertile'] = pd.qcut(density['density'], 3, labels=False)
    density = ad.obs.join(density)[['Metacell','density','tertile']]
    
    mc_density = density.groupby('Metacell').mean()[['density']].rename(columns={'density':'mc_density'})
    mc_density['mc_density_tertile'] = pd.qcut(mc_density['mc_density'], 3, labels=False)
    
    density = density.merge(mc_density, left_on='Metacell', right_index=True)
    
    if 'metacell_metrics' not in ad.uns:
        ad.uns['metacell_metrics'] = {'density':density}
    else:
        ad.uns['metacell_metrics']['density'] = density

    return density


def compute_within_metacell_entropy(ad, col_name):
    """
    Compute the entropy of the specified col_name from ad.obs within each metacell. 
    @param: ad - AnnData object with 'Metacell' assignment and col_name in ad.obs dataframe
    @param: col_name - (str) column name within ad.obs. Usually, some type of cluster or 'celltype' or similar
    """
    from collections import Counter
    from scipy.stats import entropy
    mc_ids = []
    mc_entropy = []
    for mc in ad.obs['Metacell'].unique():
        mc_ids.append(mc)
        cell_types = Counter(ad.obs[ad.obs['Metacell']==mc][col_name])

        mc_entropy.append(entropy(list(cell_types.values())))
    
    entropies = pd.DataFrame(mc_entropy)
    entropies.columns = ['entropy']
    entropies.index = mc_ids

    return entropies

